{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Algorithms:\n",
    "- **Supervised:** input data has known labels or results. <br>(*searching through a **hypothesis space** to find the **suitable hypothesis** that will make the **right predictions** for a particular problem.*)\n",
    "- **Unsupervised:** Input data has no labels/results.\n",
    "- **Semi-supervised Learning:** Input data is a mixture of labeled and unlabeled data.\n",
    "- **Reinforcement Learning:** a goal-oriented learning based on interaction with environment.(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layout/example of a ML model building: \n",
    "- **Data Preparing:** <br> 1. Analyze data.<br> 2. Descriptive stats.<br> 3. Data Visualization.\n",
    "\n",
    "- **Outliers Investigation:** <br>1. Single feature. <br>2. Pairs. \n",
    "\n",
    "- **Evaluate algorithms:** Baseline\n",
    "\n",
    "- **Feature Engineering:** <br>1. Data preprocessing (scaling, missing values, encoding). <br>2. Remove outliers.(?) \n",
    "\n",
    "- **Feature Selection:** <br>1. Correlation.(?) <br>2. Feature Importance. \n",
    "\n",
    "- **Ensemble Methods:** <br>1. Algorithm Tuning. <br>2. Voting Ensemble(?)<br> 3. Error Correlation.(?) <br>4. Stacking.(?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A means of automating machine learning workflow.\n",
    "- Enables data to be transformed and correlated into a model. \n",
    "- Splits up the workflow into independent, reusable parts that can be pipelined together to create models. \n",
    "- Cuts out redundant work.\n",
    "- Based on the idea: splitting the software into basic parts enables building more powerful software over time. \n",
    "\n",
    "#### Machine Learning workflow: \n",
    "1. ingestion. \n",
    "1. cleaning. \n",
    "1. preprocessing. \n",
    "1. modeling. \n",
    "1. deployment. <br>\n",
    "\n",
    "running these tasks using one script rises problems like: **Volume, Variety, and Versioning**. \n",
    "   \n",
    " #### benefits of pipelines: \n",
    "   - **Runtime optimization:**<br>pipelines that get heavily used can be emphasized on during deployment. This way, the right algorithms will run seamlessly and compute time will be reduced. \n",
    "   - **Famework agnositicism**:<br> since a pipeline uses API endpoints, different parts can be written in different languages and frameworks. Also allows pieces of models to be reused across the stack. \n",
    "   - **Broader applicability:**<br>\n",
    "    pieces of a model can be used for other different models. Parts can have a broad applicability.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembles\n",
    "\n",
    "- Ensemble learning uses the predictions of multiple algorithms, which turn to be better that each of the algorithms alone.\n",
    "\n",
    "\n",
    "- Ensemble algotithms usually requires **more computation**. \n",
    "\n",
    "\n",
    "- Ensemble are compensating for **poor learning** by performing **extra computation**. \n",
    "\n",
    "\n",
    "- The hypothesis of the ensemble is **not necessarily** ⊆ hypothesis space of the models it was built on. Therefore, Ensemble are more **flexible**.\n",
    "\n",
    "\n",
    "- In theory, **flexiblity** $\\implies$ **overfitting**. However, in practice techniques like **bagging**(?) reduce overfitting over training data. \n",
    "\n",
    "\n",
    "- A theoretical study showed that there is an **ideal number** of components an ensemble should have.<br>The study is called \" *the law of diminishing returns in ensemble construction* \". It suggests that having the same number of components as class labels gives the highest accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes theorem, Bayes classifiers, Naïve Bayes:\n",
    "\n",
    "- Bayes theorem describes the **probability of an event** based on the **prior knowledge** of conditions that might be related to that event.\n",
    "\n",
    "\n",
    "- Naive Bayes classifiers are based on applying Bayes theorem with a strong (naive) assumptions of independance between features.\n",
    "\n",
    "\n",
    "- Naïve Bayes classifiers are **highly scalable**: They require a **number of parameters** <u>linear</u> to **the number of features**.\n",
    "\n",
    "\n",
    "- Evaluating these classifiers is not computationally expensive. **Maximum likelihood**(?) training can be evaluated by a **closed-form expression**, which takes linear time. (unlike the **iterative approximation**(?) used by other classifiers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### references:\n",
    "   - https://algorithmia.com/blog/ml-pipeline\n",
    "   - https://www.kaggle.com/pouryaayria/a-complete-ml-pipeline-tutorial-acu-86\n",
    "   - https://en.wikipedia.org/wiki/Ensemble_learning\n",
    "   - https://en.wikipedia.org/wiki/Bayes%27_theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
